{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUpHqaBNGA39"
      },
      "source": [
        "Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB_Nvltc7JHi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "with open('drive/MyDrive/tweets.json', 'r') as f:\n",
        "  tweets = json.load(f)\n",
        "  for tweet in tweets:\n",
        "    X.append(tweet['lng'])\n",
        "    Y.append(tweet['lat'])\n",
        "\n",
        "plt.scatter(X, Y, s=5, c='black')\n",
        "\n",
        "plt.legend(\"California Tweets\")\n",
        "plt.show()\n",
        "\n",
        "t_compute = time.time() - t0\n",
        "print(t_compute)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_pnl_lUGDtg"
      },
      "source": [
        "Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRkmele5GF9J"
      },
      "outputs": [],
      "source": [
        "!pip install haversine\n",
        "from haversine import haversine\n",
        "\n",
        "my_point = (34.212730341331145, -118.2859001567625)\n",
        "\n",
        "counter = 0\n",
        "radius = 50000\n",
        "for tweet in tweets:\n",
        "  point = (tweet['lat'], tweet['lng'])\n",
        "  dist = haversine(my_point, point, unit='m')\n",
        "  if dist <= radius:\n",
        "    counter += 1\n",
        "\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k5EzLENha2d"
      },
      "source": [
        "Part 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLiN4ob5hcEz"
      },
      "outputs": [],
      "source": [
        "# Setting the minimum lat & lng as origin\n",
        "from haversine import haversine\n",
        "\n",
        "x0 = 200\n",
        "y0 = 200\n",
        "\n",
        "for tweet in tweets:\n",
        "  if tweet['lat'] < x0:\n",
        "    x0 = tweet['lat']\n",
        "  if tweet['lng'] < y0:\n",
        "    y0 = tweet['lng']\n",
        "\n",
        "# Converting lat & lng data to x & y\n",
        "converted_tweets = []\n",
        "for tweet in tweets:\n",
        "  x = haversine((x0, 0), (tweet['lat'], 0), unit='m')\n",
        "  y = haversine((0, y0), (0, tweet['lng']), unit='m')\n",
        "  converted_tweets.append([x, y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U89OApRTsg8K"
      },
      "outputs": [],
      "source": [
        "# KMeans\n",
        "import time\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k_means = KMeans(init=\"k-means++\", n_clusters=100, n_init=10)\n",
        "t0 = time.time()\n",
        "k_means.fit(converted_tweets)\n",
        "t_batch = time.time() - t0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n15ALhMlyA66"
      },
      "outputs": [],
      "source": [
        "# Optimum k for KMeans\n",
        "step = 100\n",
        "k = 100\n",
        "while k < 10000:\n",
        "  k_means = KMeans(init=\"k-means++\", n_clusters=k, n_init=10)\n",
        "  t0 = time.time()\n",
        "  k_means.fit(converted_tweets)\n",
        "  t = time.time() - t0\n",
        "  print(\"%d %.2f %d\" % (k, t, k_means.inertia_))\n",
        "  k = int(k + step)\n",
        "  step = (step + 100) * 1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFOVzPjdrCnz"
      },
      "outputs": [],
      "source": [
        "# Mini Batch KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "mbk = MiniBatchKMeans(\n",
        "  init=\"k-means++\",\n",
        "  n_clusters=100,\n",
        "  batch_size=10000,\n",
        "  n_init=10,\n",
        "  max_no_improvement=100,\n",
        "  verbose=0,\n",
        ")\n",
        "t0 = time.time()\n",
        "mbk.fit(converted_tweets)\n",
        "t_mini_batch = time.time() - t0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtT2-qjc0Eom"
      },
      "outputs": [],
      "source": [
        "# Optimum k for Mini Batch KMeans\n",
        "step = 100\n",
        "k = 100\n",
        "while k < 10000:\n",
        "  mbk = MiniBatchKMeans(\n",
        "    init=\"k-means++\",\n",
        "    n_clusters=k,\n",
        "    batch_size=10000,\n",
        "    n_init=10,\n",
        "    max_no_improvement=100,\n",
        "    verbose=0,\n",
        "  )\n",
        "  t0 = time.time()\n",
        "  mbk.fit(converted_tweets)\n",
        "  t = time.time() - t0\n",
        "  print(\"%d %.2f %d\" % (k, t, mbk.inertia_))\n",
        "  k = int(k + step)\n",
        "  step = (step + 100) * 1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlTNNuCprbFw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "k_means_cluster_centers = k_means.cluster_centers_\n",
        "order = pairwise_distances_argmin(k_means.cluster_centers_, mbk.cluster_centers_)\n",
        "mbk_means_cluster_centers = mbk.cluster_centers_[order]\n",
        "\n",
        "k_means_labels = pairwise_distances_argmin(converted_tweets, k_means_cluster_centers)\n",
        "mbk_means_labels = pairwise_distances_argmin(converted_tweets, mbk_means_cluster_centers)\n",
        "\n",
        "fig = plt.figure(figsize=(8, 3))\n",
        "fig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)\n",
        "colors = [\"#\" + ''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
        "  for i in range(100)]\n",
        "\n",
        "# KMeans\n",
        "converted_tweets = np.array(converted_tweets)\n",
        "ax = fig.add_subplot(1, 3, 1)\n",
        "for k, col in zip(range(100), colors):\n",
        "  my_members = k_means_labels == k\n",
        "  cluster_center = k_means_cluster_centers[k]\n",
        "  ax.plot(\n",
        "    converted_tweets[my_members, 0],\n",
        "    converted_tweets[my_members, 1],\n",
        "    \"w\",\n",
        "    markerfacecolor=col,\n",
        "    marker=\".\"\n",
        "  )\n",
        "  ax.plot(\n",
        "    cluster_center[0],\n",
        "    cluster_center[1],\n",
        "    \"o\",\n",
        "    markerfacecolor=col,\n",
        "    markeredgecolor=\"k\",\n",
        "    markersize=6,\n",
        "  )\n",
        "ax.set_title(\"KMeans\")\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "plt.text(-3.5, 1.8, \"train time: %.2fs\" % (t_batch))\n",
        "\n",
        "# MiniBatchKMeans\n",
        "ax = fig.add_subplot(1, 3, 2)\n",
        "for k, col in zip(range(100), colors):\n",
        "  my_members = mbk_means_labels == k\n",
        "  cluster_center = mbk_means_cluster_centers[k]\n",
        "  ax.plot(\n",
        "    converted_tweets[my_members, 0],\n",
        "    converted_tweets[my_members, 1],\n",
        "    \"w\",\n",
        "    markerfacecolor=col,\n",
        "    marker=\".\"\n",
        "  )\n",
        "  ax.plot(\n",
        "    cluster_center[0],\n",
        "    cluster_center[1],\n",
        "    \"o\",\n",
        "    markerfacecolor=col,\n",
        "    markeredgecolor=\"k\",\n",
        "    markersize=6,\n",
        "  )\n",
        "ax.set_title(\"MiniBatchKMeans\")\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "plt.text(-3.5, 1.8, \"train time: %.2fs\" % (t_mini_batch))\n",
        "\n",
        "# Initialize the different array to all False\n",
        "different = mbk_means_labels == 4\n",
        "ax = fig.add_subplot(1, 3, 3)\n",
        "\n",
        "for k in range(100):\n",
        "  different += (k_means_labels == k) != (mbk_means_labels == k)\n",
        "\n",
        "identic = np.logical_not(different)\n",
        "ax.plot(\n",
        "  converted_tweets[identic, 0],\n",
        "  converted_tweets[identic, 1],\n",
        "  \"w\",\n",
        "  markerfacecolor=\"#bbbbbb\",\n",
        "  marker=\".\"\n",
        ")\n",
        "ax.plot(\n",
        "  converted_tweets[different, 0],\n",
        "  converted_tweets[different, 1],\n",
        "  \"w\",\n",
        "  markerfacecolor=\"m\",\n",
        "  marker=\".\"\n",
        ")\n",
        "ax.set_title(\"Difference\")\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDV7-xqG0eBI"
      },
      "outputs": [],
      "source": [
        "# DBScan\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "centers = 100\n",
        "\n",
        "db = DBSCAN(eps=1000, min_samples=100).fit(converted_tweets)\n",
        "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "core_samples_mask[db.core_sample_indices_] = True\n",
        "labels = db.labels_\n",
        "\n",
        "# Number of clusters in labels, ignoring noise if present.\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)\n",
        "\n",
        "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
        "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
        "\n",
        "# Black removed and is used for noise instead.\n",
        "unique_labels = set(labels)\n",
        "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
        "\n",
        "for k, col in zip(unique_labels, colors):\n",
        "  if k == -1:\n",
        "    # noise color\n",
        "    col = [0, 0, 0, 1]\n",
        "\n",
        "  class_member_mask = labels == k\n",
        "\n",
        "  xy = converted_tweets[class_member_mask & ~core_samples_mask]\n",
        "  plt.plot(\n",
        "    xy[:, 0],\n",
        "    xy[:, 1],\n",
        "    \"o\",\n",
        "    markerfacecolor=tuple(col),\n",
        "    markeredgecolor=\"k\",\n",
        "    markersize=6,\n",
        "    zorder=0\n",
        "  )\n",
        "\n",
        "  xy = converted_tweets[class_member_mask & core_samples_mask]\n",
        "  plt.plot(\n",
        "    xy[:, 0],\n",
        "    xy[:, 1],\n",
        "    \"o\",\n",
        "    markerfacecolor=tuple(col),\n",
        "    markeredgecolor=\"k\",\n",
        "    markersize=14,\n",
        "    zorder=5\n",
        "  )\n",
        "\n",
        "plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGawZQ-fI4oi"
      },
      "outputs": [],
      "source": [
        "centers = 100\n",
        "eps = 10\n",
        "while eps <= 2000:\n",
        "  t0 = time.time()\n",
        "  db = DBSCAN(eps=eps, min_samples=100).fit(converted_tweets)\n",
        "  core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "  core_samples_mask[db.core_sample_indices_] = True\n",
        "  labels = db.labels_\n",
        "\n",
        "  # Number of clusters in labels, ignoring noise if present.\n",
        "  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "  n_noise_ = list(labels).count(-1)\n",
        "  t_exe = time.time() - t0\n",
        "  print(\"%d %.2f %d %d\" % (eps, t_exe, n_clusters_, n_noise_))\n",
        "  eps = eps + 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHSfIqlvShVd"
      },
      "source": [
        "Part 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47SzzdIKSjUG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "no = 100\n",
        "step = 100\n",
        "while no <= 100000:\n",
        "  samples = random.sample(np.array(converted_tweets).tolist(), no)\n",
        "  k_means = KMeans(init=\"k-means++\", n_clusters=100, n_init=10)\n",
        "  t0 = time.time()\n",
        "  k_means.fit(samples)\n",
        "  t = time.time() - t0\n",
        "  print(\"%d %.2f %d\" % (no, t, k_means.inertia_))\n",
        "  no = int(no + step)\n",
        "  step = (step + 100) * 1.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STrwEHmIZ3Jt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "no = 100\n",
        "step = 100\n",
        "while no <= 100000:\n",
        "  samples = random.sample(np.array(converted_tweets).tolist(), no)\n",
        "  mbk = MiniBatchKMeans(\n",
        "    init=\"k-means++\",\n",
        "    n_clusters=100,\n",
        "    batch_size=int(no / 10),\n",
        "    n_init=10,\n",
        "    max_no_improvement=100,\n",
        "    verbose=0,\n",
        "  )\n",
        "  t0 = time.time()\n",
        "  mbk.fit(samples)\n",
        "  t = time.time() - t0\n",
        "  print(\"%d %.2f %d\" % (no, t, mbk.inertia_))\n",
        "  no = int(no + step)\n",
        "  step = (step + 100) * 1.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs6rT6N3hB3G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "no = 100\n",
        "step = 100\n",
        "\n",
        "while no <= 100000:\n",
        "  samples = random.sample(np.array(converted_tweets).tolist(), no)\n",
        "  t0 = time.time()\n",
        "  db = DBSCAN(eps=320, min_samples=100).fit(samples)\n",
        "  core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "  core_samples_mask[db.core_sample_indices_] = True\n",
        "  labels = db.labels_\n",
        "  t_exe = time.time() - t0\n",
        "\n",
        "  # Number of clusters in labels, ignoring noise if present.\n",
        "  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "  n_noise_ = list(labels).count(-1)\n",
        "  print(\"%d %.2f %d %d\" % (no, t_exe, n_clusters_, n_noise_))\n",
        "\n",
        "  no = int(no + step)\n",
        "  step = (step + 100) * 1.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E4nOcuEjU0s"
      },
      "source": [
        "Part 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DC8jYF0jWbw"
      },
      "outputs": [],
      "source": [
        "# Mini Batch KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import math\n",
        "\n",
        "mbk = MiniBatchKMeans(\n",
        "  init=\"k-means++\",\n",
        "  n_clusters=100,\n",
        "  batch_size=10000,\n",
        "  n_init=10,\n",
        "  max_no_improvement=100,\n",
        "  verbose=0,\n",
        ")\n",
        "t0 = time.time()\n",
        "mbk.fit(converted_tweets)\n",
        "t_mini_batch = time.time() - t0\n",
        "\n",
        "def get_distance(point1, point2):\n",
        "  return math.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
        "\n",
        "cluster_centers = []\n",
        "my_point = [34.212730341331145, -118.2859001567625]\n",
        "# Calc distances from my_point\n",
        "for index, cluster_center in enumerate(mbk.cluster_centers_):\n",
        "  cluster_centers.append([index, get_distance(my_point, cluster_center)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZIHMvEOoKXn"
      },
      "outputs": [],
      "source": [
        "# Sort clusters regarding their distance from my_point\n",
        "cluster_centers.sort(key=lambda x: x[1])\n",
        "sliced_clusters = cluster_centers[0: 5]\n",
        "sliced_clusters_indexes = [index for index, value in sliced_clusters]\n",
        "cluster_tweets = [x for index, x in enumerate(tweets) if mbk.labels_[index] in sliced_clusters_indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gfp9fdwgsIYX"
      },
      "outputs": [],
      "source": [
        "used_words = []\n",
        "words_count = []\n",
        "redundant_chars = ['.', ',', ';', '!', '?', '\\n', '\\r', '\"', '\\'', '#', '-', '(', ')']\n",
        "\n",
        "for tweet in cluster_tweets:\n",
        "  text = tweet['text'].lower()\n",
        "  for redundant_char in redundant_chars:\n",
        "    text = text.replace(redundant_char, '')\n",
        "  words = text.split(' ')\n",
        "  for word in words:\n",
        "    # Remove redundant parts from words\n",
        "    if word in used_words:\n",
        "      index = used_words.index(word)\n",
        "      words_count[index][1] += 1\n",
        "    else:\n",
        "      used_words.append(word)\n",
        "      words_count.append([word, 1])\n",
        "\n",
        "words_count.sort(key=lambda x: x[1], reverse=True)\n",
        "print(words_count)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}